{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NeuralNetwork(object):\n",
    "  def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "    assert(len(layers) == len(activations)+1)\n",
    "    self.layers = layers\n",
    "    self.activations = activations\n",
    "    self.weights = []\n",
    "    self.biases = []\n",
    "    for i in range(len(layers)-1):\n",
    "      self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
    "      self.biases.append(np.random.randn(layers[i+1], 1))\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "      # return the feedforward values Z and A vectors for all\n",
    "      # layers\n",
    "     return (z_s, a_s)    \n",
    "\n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "      # get target values and feedforward data \n",
    "      # return the derivitives respect to weight matrix and biases\n",
    "      return dw, db\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "      # update weights and biases based on the output\n",
    "      for e in range(epochs): \n",
    "        i=0\n",
    "        while(i<len(y)):\n",
    "          x_batch = x[i:i+batch_size]\n",
    "          y_batch = y[i:i+batch_size]\n",
    "          i = i+batch_size\n",
    "          z_s, a_s = self.feedforward(x_batch)\n",
    "          dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "          self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "          self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "          print(\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, x):\n",
    "  # return the feedforward value for x\n",
    "  a = np.copy(x)\n",
    "  z_s = []\n",
    "  a_s = [a]\n",
    "  for i in range(len(self.weights)):\n",
    "    activation_function = self.getActivationFunction(self.activations[i])\n",
    "    z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "    a = activation_function(z_s[-1])\n",
    "    a_s.append(a)\n",
    "    return (z_s, a_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def getActivationFunction(name):\n",
    "  if(name == 'sigmoid'):\n",
    "    return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "  elif(name == 'linear'):\n",
    "    return lambda x : x\n",
    "  elif(name == 'relu'):\n",
    "    def relu(x):\n",
    "      y = np.copy(x)\n",
    "      y[y<0] = 0\n",
    "      return y\n",
    "    return relu\n",
    "  else:\n",
    "    print('Unknown activation function. linear is used')\n",
    "    return lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self,y, z_s, a_s):\n",
    "  dw = []  # dC/dW\n",
    "  db = []  # dC/dB\n",
    "  deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
    "  # insert the last layer error\n",
    "  deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "  # Perform BackPropagation\n",
    "  for i in reversed(range(len(deltas)-1)):\n",
    "    deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "    batch_size = y.shape[1]\n",
    "    db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "    dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "    # return the derivitives respect to weight matrix and biases\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def getDerivitiveActivationFunction(name):\n",
    "    if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "    elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "    elif(name == 'relu'):          \n",
    "        def relu_diff(x):\n",
    "            y = np.copy(x)\n",
    "            y[y>=0] = 1\n",
    "            y[y<0] = 0\n",
    "            return y\n",
    "        return relu_diff\n",
    "    else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dd930c679437>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    nn = NeuralNetwork([1, 100, 1],activations=['sigmoid', 'sigmoid'])\n",
    "    X = 2*np.pi*np.random.rand(1000).reshape(1, -1)\n",
    "    y = np.sin(X)\n",
    "    \n",
    "nn.train(X, y, epochs=10000, batch_size=64, lr = .1)\n",
    "_, a_s = nn.feedforward(X)\n",
    "print(y, X)\n",
    "    \n",
    "plt.scatter(X.flatten(), y.flatten())\n",
    "plt.scatter(X.flatten(), a_s[-1].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
